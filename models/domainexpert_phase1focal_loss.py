# -*- coding: utf-8 -*-
"""Domainexpert_phase1focal_loss

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R3spkijxS6oA4wF-OhUflrxPfhODCZ0M

#librerie
"""

!pip install --upgrade pip
!pip install --upgrade transformers>=4.30.0

"""# Read data

"""

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/auchanfrance/produits_nettoyes.csv", sep=';', on_bad_lines='skip')
df.head(5)

"""# Data augmentation"""

import random
import math
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
import torch
import torch.nn as nn


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score, accuracy_score
from sklearn.utils.class_weight import compute_class_weight

# ----------------------------
# CONFIG
# ----------------------------
DATA_PATH = "/content/drive/MyDrive/auchanfrance/produits_nettoyes.csv"

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

SEED = 42
MAX_LEN = 160
BATCH_SIZE = 4               # small because xlm-roberta-large heavy; we use grad accumulation
GRAD_ACCUM_STEPS = 8         # effective batch size = BATCH_SIZE * GRAD_ACCUM_STEPS
NUM_EPOCHS = 6
BASE_LR = 2e-5
WEIGHT_DECAY = 0.01
WARMUP_PCT = 0.05
NUM_WORKERS = 2
RARE_THRESH = 50             # seuil pour consid√©rer "rare"
PATIENCE = 3                 # early stopping patience (val F1)

# reproducibility
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# ----------------------------
# 1) Read dataset & encode labels
# ----------------------------
df = pd.read_csv(DATA_PATH, sep=';', on_bad_lines='skip')
df['description'] = df['description'].astype(str)
df['taxonomy_path'] = df['taxonomy_path'].astype(str)

texts = df['description'].tolist()
labels_raw = df['taxonomy_path'].tolist()

le = LabelEncoder()
labels = le.fit_transform(labels_raw)
num_classes = len(le.classes_)
print(f"Dataset total : {len(df)} lignes")
print(f"Nombre de labels uniques : {num_classes}")

# ----------------------------
# 2) Stratified split + dedup val/test
# ----------------------------
X_train, X_temp, y_train, y_temp = train_test_split(
    texts, labels, test_size=0.30, random_state=SEED, stratify=labels
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=SEED, stratify=y_temp
)

train_df = pd.DataFrame({"description": X_train, "taxonomy_path": le.inverse_transform(y_train)})
val_df   = pd.DataFrame({"description": X_val,   "taxonomy_path": le.inverse_transform(y_val)})
test_df  = pd.DataFrame({"description": X_test,  "taxonomy_path": le.inverse_transform(y_test)})

KEY = ["description", "taxonomy_path"]
val_df = val_df.drop_duplicates(subset=KEY, keep="first").reset_index(drop=True)
test_df = test_df.drop_duplicates(subset=KEY, keep="first").reset_index(drop=True)

print(f"Taille val apr√®s d√©doublonnage : {len(val_df)}")
print(f"Taille test apr√®s d√©doublonnage : {len(test_df)}")

# reconstruct lists (train kept duplicates intentionally)
X_train = train_df['description'].tolist()
y_train = le.transform(train_df['taxonomy_path'])
X_val   = val_df['description'].tolist()
y_val   = le.transform(val_df['taxonomy_path'])
X_test  = test_df['description'].tolist()
y_test  = le.transform(test_df['taxonomy_path'])

# ----------------------------
# 3) Augmentation classes rares (garder ta m√©thode)
# ----------------------------
counts = pd.Series(y_train).value_counts()
rare_classes = counts[counts < RARE_THRESH].index.tolist()
print(f"Classes rares (<{RARE_THRESH}) : {len(rare_classes)}")

def augment_texts_weighted(text, taxonomy, n_masks=2):
    # keep original + full taxonomy + masked variants
    out = [f"{text} | Category path: {taxonomy}"]
    parts = taxonomy.split(" > ")
    for _ in range(n_masks):
        if len(parts) > 1:
            idx = np.random.randint(0, len(parts))
            masked = [parts[i] if i != idx else "[MASK]" for i in range(len(parts))]
            out.append(f"{text} | Category path: {' > '.join(masked)}")
    return out

aug_texts = list(X_train)
aug_labels = list(y_train)

for txt, lbl in zip(X_train, y_train):
    if lbl in rare_classes:
        taxonomy = le.inverse_transform([lbl])[0]
        new_versions = augment_texts_weighted(txt, taxonomy, n_masks=3)
        aug_texts.extend(new_versions)
        aug_labels.extend([lbl] * len(new_versions))

X_train_aug = aug_texts
y_train_aug = aug_labels
print("Taille train apr√®s augmentation :", len(X_train_aug))

import os

# ----------------------------
# 4) Cr√©er le dossier de sortie
# ----------------------------
OUTPUT_DIR = "dataauchan"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ----------------------------
# 5) Pr√©parer les DataFrames pour sauvegarde
# ----------------------------
train_df_aug = pd.DataFrame({
    "description": X_train_aug,
    "taxonomy_path": le.inverse_transform(y_train_aug)
})

val_df_clean = pd.DataFrame({
    "description": X_val,
    "taxonomy_path": le.inverse_transform(y_val)
})

test_df_clean = pd.DataFrame({
    "description": X_test,
    "taxonomy_path": le.inverse_transform(y_test)
})

# ----------------------------
# 6) Sauvegarder au format CSV
# ----------------------------
train_path = os.path.join(OUTPUT_DIR, "train.csv")
val_path   = os.path.join(OUTPUT_DIR, "val.csv")
test_path  = os.path.join(OUTPUT_DIR, "test.csv")

train_df_aug.to_csv(train_path, index=False, sep=';')
val_df_clean.to_csv(val_path, index=False, sep=';')
test_df_clean.to_csv(test_path, index=False, sep=';')

print(f"‚úÖ Train sauvegard√© : {train_path} ({len(train_df_aug)} lignes)")
print(f"‚úÖ Val sauvegard√©   : {val_path} ({len(val_df_clean)} lignes)")
print(f"‚úÖ Test sauvegard√©  : {test_path} ({len(test_df_clean)} lignes)")

"""# ‚úÖ Train sauvegard√© : dataauchan/train.csv (110266 lignes)
# ‚úÖ Val sauvegard√©   : dataauchan/val.csv (9582 lignes)
# ‚úÖ Test sauvegard√©  : dataauchan/test.csv (9589 lignes)


"""

import shutil
import os

# Chemin local
local_folder = "dataauchan"

# Chemin sur ton Drive (adapte si n√©cessaire)
drive_folder = "/content/drive/MyDrive/dataauchan"
os.makedirs(drive_folder, exist_ok=True)

# Copier le dossier entier
shutil.copytree(local_folder, drive_folder, dirs_exist_ok=True)

print(f"‚úÖ Dossier copi√© dans Drive : {drive_folder}")

"""# Phase 1:Focal loss"""

# -*- coding: utf-8 -*-
"""
Phase 2 - Domain Expert fine-tune
XLM-RoBERTa Large + Label augmentation (multi-mask) + Focal Loss + Weighted Sampler
Mixed precision, gradient accumulation, scheduler, per-class accuracy reporting.
Designed for many classes (‚âà882).
"""

import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"

import random
import math
import numpy as np
import pandas as pd
from tqdm.auto import tqdm

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
from transformers import (
    XLMRobertaTokenizerFast,
    XLMRobertaModel,
    get_linear_schedule_with_warmup
)
from torch.optim import AdamW

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score, accuracy_score
from sklearn.utils.class_weight import compute_class_weight

# ----------------------------
# CONFIG
# ----------------------------
DATA_PATH = "/content/drive/MyDrive/auchanfrance/produits_nettoyes.csv"   # adapte si besoin
PHASE1_CKPT = "/content/drive/MyDrive/best-model-phase1-foca-loss/best_domain_expertfl_model.pth"  # optional
OUTPUT_DIR = "./best_domain_expert_phase1fl_xlmlarge"
os.makedirs(OUTPUT_DIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE)

SEED = 42
MAX_LEN = 160
BATCH_SIZE = 4               # small because xlm-roberta-large heavy; we use grad accumulation
GRAD_ACCUM_STEPS = 8         # effective batch size = BATCH_SIZE * GRAD_ACCUM_STEPS
NUM_EPOCHS = 10
BASE_LR = 2e-5
WEIGHT_DECAY = 0.01
WARMUP_PCT = 0.05
NUM_WORKERS = 2
RARE_THRESH = 50             # seuil pour consid√©rer "rare"
PATIENCE = 3                 # early stopping patience (val F1)
SAVE_NAME = os.path.join(OUTPUT_DIR, "best_domain_expert_xlmlarge.pth")

# reproducibility
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# ----------------------------
# 1) Read dataset & encode labels
# ----------------------------
df = pd.read_csv(DATA_PATH, sep=';', on_bad_lines='skip')
df['description'] = df['description'].astype(str)
df['taxonomy_path'] = df['taxonomy_path'].astype(str)

texts = df['description'].tolist()
labels_raw = df['taxonomy_path'].tolist()

le = LabelEncoder()
labels = le.fit_transform(labels_raw)
num_classes = len(le.classes_)
print(f"Dataset total : {len(df)} lignes")
print(f"Nombre de labels uniques : {num_classes}")

# ----------------------------
# 2) Stratified split + dedup val/test
# ----------------------------
X_train, X_temp, y_train, y_temp = train_test_split(
    texts, labels, test_size=0.30, random_state=SEED, stratify=labels
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, random_state=SEED, stratify=y_temp
)

train_df = pd.DataFrame({"description": X_train, "taxonomy_path": le.inverse_transform(y_train)})
val_df   = pd.DataFrame({"description": X_val,   "taxonomy_path": le.inverse_transform(y_val)})
test_df  = pd.DataFrame({"description": X_test,  "taxonomy_path": le.inverse_transform(y_test)})

KEY = ["description", "taxonomy_path"]
val_df = val_df.drop_duplicates(subset=KEY, keep="first").reset_index(drop=True)
test_df = test_df.drop_duplicates(subset=KEY, keep="first").reset_index(drop=True)

print(f"Taille val apr√®s d√©doublonnage : {len(val_df)}")
print(f"Taille test apr√®s d√©doublonnage : {len(test_df)}")

# reconstruct lists (train kept duplicates intentionally)
X_train = train_df['description'].tolist()
y_train = le.transform(train_df['taxonomy_path'])
X_val   = val_df['description'].tolist()
y_val   = le.transform(val_df['taxonomy_path'])
X_test  = test_df['description'].tolist()
y_test  = le.transform(test_df['taxonomy_path'])

# ----------------------------
# 3) Augmentation classes rares (garder ta m√©thode)
# ----------------------------
counts = pd.Series(y_train).value_counts()
rare_classes = counts[counts < RARE_THRESH].index.tolist()
print(f"Classes rares (<{RARE_THRESH}) : {len(rare_classes)}")

def augment_texts_weighted(text, taxonomy, n_masks=2):
    # keep original + full taxonomy + masked variants
    out = [f"{text} | Category path: {taxonomy}"]
    parts = taxonomy.split(" > ")
    for _ in range(n_masks):
        if len(parts) > 1:
            idx = np.random.randint(0, len(parts))
            masked = [parts[i] if i != idx else "[MASK]" for i in range(len(parts))]
            out.append(f"{text} | Category path: {' > '.join(masked)}")
    return out

aug_texts = list(X_train)
aug_labels = list(y_train)

for txt, lbl in zip(X_train, y_train):
    if lbl in rare_classes:
        taxonomy = le.inverse_transform([lbl])[0]
        new_versions = augment_texts_weighted(txt, taxonomy, n_masks=3)
        aug_texts.extend(new_versions)
        aug_labels.extend([lbl] * len(new_versions))

X_train_aug = aug_texts
y_train_aug = aug_labels
print("Taille train apr√®s augmentation :", len(X_train_aug))

# ----------------------------
# 4) Tokenizer + Dataset
# ----------------------------
tokenizer = XLMRobertaTokenizerFast.from_pretrained("xlm-roberta-large")

class ECommDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=MAX_LEN):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        enc = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_len,
            return_attention_mask=True,
            return_tensors="pt"
        )
        return {
            "input_ids": enc["input_ids"].squeeze(0),
            "attention_mask": enc["attention_mask"].squeeze(0),
            "labels": torch.tensor(self.labels[idx], dtype=torch.long)
        }

train_ds = ECommDataset(X_train_aug, y_train_aug, tokenizer)
val_ds   = ECommDataset(X_val, y_val, tokenizer)
test_ds  = ECommDataset(X_test, y_test, tokenizer)

# ----------------------------
# 5) Weighted sampler to rebalance rare classes during training
# ----------------------------
class_counts_full = np.bincount(y_train_aug, minlength=num_classes)
# avoid zeros
class_counts_full = np.where(class_counts_full == 0, 1, class_counts_full)
sample_weights = np.array([1.0 / class_counts_full[label] for label in y_train_aug])
sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=NUM_WORKERS)
val_loader   = DataLoader(val_ds, batch_size=32, shuffle=False, num_workers=NUM_WORKERS)
test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=NUM_WORKERS)

# ----------------------------
# 6) Model (XLM-R large) + optional load Phase1 checkpoint
# ----------------------------
class DomainExpert(nn.Module):
    def __init__(self, n_classes, dropout=0.3):
        super().__init__()
        self.xlm = XLMRobertaModel.from_pretrained("xlm-roberta-large")
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(self.xlm.config.hidden_size, n_classes)
    def forward(self, input_ids, attention_mask):
        out = self.xlm(input_ids=input_ids, attention_mask=attention_mask)
        cls = out.last_hidden_state[:, 0, :]            # CLS token
        return self.classifier(self.dropout(cls))

model = DomainExpert(num_classes).to(DEVICE)

# try load Phase1 checkpoint if available (strict=False)
if os.path.exists(PHASE1_CKPT):
    try:
        ckpt = torch.load(PHASE1_CKPT, map_location="cpu")
        model.load_state_dict(ckpt, strict=False)
        print("‚úÖ Phase1 checkpoint loaded with strict=False")
    except Exception as e:
        print("‚ö† Could not fully load Phase1 checkpoint:", e)

# ----------------------------
# 7) Focal Loss (stable) + class weights
# ----------------------------
# compute class weights to use as alpha in focal loss (balanced)
weights_np = compute_class_weight(class_weight="balanced", classes=np.arange(num_classes), y=np.array(y_train_aug))
alpha_tensor = torch.tensor(weights_np, dtype=torch.float).to(DEVICE)

class FocalLossStable(nn.Module):
    def __init__(self, alpha=None, gamma=2.0, reduction="mean", eps=1e-9):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        self.eps = eps
        self.ce = nn.CrossEntropyLoss(reduction="none")
    def forward(self, logits, targets):
        # logits: [B, C], targets: [B]
        ce_loss = self.ce(logits, targets)                     # per-sample CE
        pt = torch.softmax(logits, dim=1).gather(1, targets.unsqueeze(1)).squeeze(1).clamp(min=self.eps)
        if self.alpha is not None:
            alpha_factor = self.alpha.gather(0, targets)       # [B]
            focal = alpha_factor * ((1.0 - pt) ** self.gamma) * ce_loss
        else:
            focal = ((1.0 - pt) ** self.gamma) * ce_loss
        if self.reduction == "mean":
            return focal.mean()
        elif self.reduction == "sum":
            return focal.sum()
        else:
            return focal

criterion = FocalLossStable(alpha=alpha_tensor, gamma=2.0, reduction="mean")

# ----------------------------
# 8) Optimizer, scheduler, AMP setup
# ----------------------------
optimizer = AdamW(model.parameters(), lr=BASE_LR, weight_decay=WEIGHT_DECAY)
total_steps = len(train_loader) * NUM_EPOCHS // max(1, GRAD_ACCUM_STEPS)
warmup_steps = max(1, int(total_steps * WARMUP_PCT))
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)

scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE=="cuda"))

# ----------------------------
# 9) TRAIN loop with mixed precision + grad accumulation + val monitoring
# ----------------------------
best_f1 = 0.0
no_improve = 0
progress = {"train_loss": [], "val_f1": []}

for epoch in range(1, NUM_EPOCHS + 1):
    model.train()
    running_loss = 0.0
    pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f"Epoch {epoch}/{NUM_EPOCHS} [Train]")
    optimizer.zero_grad()
    for step, batch in pbar:
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        labels_b = batch["labels"].to(DEVICE)

        with torch.cuda.amp.autocast(enabled=(DEVICE=="cuda")):
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels_b) / GRAD_ACCUM_STEPS

        scaler.scale(loss).backward()

        if (step + 1) % GRAD_ACCUM_STEPS == 0 or (step + 1) == len(train_loader):
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            scheduler.step()

        running_loss += loss.item() * GRAD_ACCUM_STEPS
        avg_loss = running_loss / (step + 1)
        pbar.set_postfix({"avg_loss": f"{avg_loss:.4f}"})

    progress["train_loss"].append(avg_loss)

    # ---------- VALID ----------
    model.eval()
    all_preds, all_trues = [], []
    val_loss = 0.0
    with torch.no_grad():
        vbar = tqdm(val_loader, desc=f"Epoch {epoch} [Val]", leave=False)
        for batch in vbar:
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels_b = batch["labels"].to(DEVICE)
            with torch.cuda.amp.autocast(enabled=(DEVICE=="cuda")):
                logits = model(input_ids, attention_mask)
                loss = criterion(logits, labels_b)
            val_loss += loss.item()
            preds = torch.argmax(logits, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_trues.extend(labels_b.cpu().numpy())
            vbar.set_postfix({"batch_loss": f"{loss.item():.4f}"})

    val_loss /= max(1, len(val_loader))
    val_f1 = f1_score(all_trues, all_preds, average="macro")
    val_acc = accuracy_score(all_trues, all_preds)
    progress["val_f1"].append(val_f1)

    print(f"\nEpoch {epoch} summary -> TrainLoss: {avg_loss:.4f} | ValLoss: {val_loss:.4f} | ValAcc: {val_acc:.4f} | ValF1: {val_f1:.4f}")

    # save best
    if val_f1 > best_f1:
        best_f1 = val_f1
        no_improve = 0
        torch.save(model.state_dict(), SAVE_NAME)
        print("üåü New best saved:", SAVE_NAME)
    else:
        no_improve += 1
        print(f"No improvement: {no_improve}/{PATIENCE}")
        if no_improve >= PATIENCE:
            print("Early stopping triggered.")
            break

# ----------------------------
# 10) Final evaluation on test set + per-class accuracy
# ----------------------------
if os.path.exists(SAVE_NAME):
    model.load_state_dict(torch.load(SAVE_NAME, map_location=DEVICE))
    print("Loaded best model for final evaluation.")

model.eval()
all_preds, all_trues = [], []
with torch.no_grad():
    tbar = tqdm(test_loader, desc="Test Eval")
    for batch in tbar:
        input_ids = batch["input_ids"].to(DEVICE)
        attention_mask = batch["attention_mask"].to(DEVICE)
        labels_b = batch["labels"].to(DEVICE)
        logits = model(input_ids, attention_mask)
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_trues.extend(labels_b.cpu().numpy())

test_f1 = f1_score(all_trues, all_preds, average="macro")
test_acc = accuracy_score(all_trues, all_preds)
print("\n========== FINAL RESULTS ==========")
print(f"Test Accuracy : {test_acc:.4f}")
print(f"Test F1-macro : {test_f1:.4f}")

# per-class accuracy
all_preds = np.array(all_preds)
all_trues = np.array(all_trues)
per_class_acc = []
for c in range(num_classes):
    mask = (all_trues == c)
    if mask.sum() == 0:
        per_class_acc.append(np.nan)
    else:
        per_class_acc.append((all_preds[mask] == all_trues[mask]).mean())

pc_df = pd.DataFrame({
    "class_id": np.arange(num_classes),
    "class_name": [le.inverse_transform([i])[0] for i in range(num_classes)],
    "acc": per_class_acc,
    "support": [(all_trues == i).sum() for i in range(num_classes)]
})
# sort by worst accuracy (excluding NaN)
pc_df_valid = pc_df.dropna().sort_values("acc")
print("\nWorst 10 classes by accuracy:")
print(pc_df_valid.head(10)[["class_id", "class_name", "support", "acc"]].to_string(index=False))

# save per-class report
pc_df.to_csv(os.path.join(OUTPUT_DIR, "per_class_accuracy.csv"), index=False)
print("Per-class accuracy saved ->", os.path.join(OUTPUT_DIR, "per_class_accuracy.csv"))